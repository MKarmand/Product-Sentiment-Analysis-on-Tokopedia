# -*- coding: utf-8 -*-
"""Another copy of AnalisisSentimenMLP(Sharing).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17J1QNbBQIAO8Lzil4Qgj84mN6T9hQXV6

# Preprocessing

Import Library
"""

pip install emoji

import pandas as pd
import numpy as np
import re
import string
import nltk
import matplotlib.pyplot as plt
import joblib
import emoji
import random
import seaborn as sns
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

nltk.download('punkt_tab')
nltk.download('stopwords')
print(nltk.data.path)

# menetapkan random supaya hasil tidak acak
random.seed(42)
np.random.seed(42)

"""Load Data"""

# Membaca file dengan nama spesifik
df = pd.read_csv("data.csv")
df

# Menampilkan data duplikat
df[df.duplicated()]

"""- Ganti kata slang dengan kata baku
- hapus Kata tidak relevan
- Mengubah huruf menjadi huruf kecil
- hapus angka
- hapus tanda baca
- cleaning data
"""

# Kamus penggantian kata-kata slang menjadi baku dan penggantian kata bahsa inggris menjadi indonesia
custom_dict = {
    'mantap': ['mantep'],
    'bagus' : ['good', 'top'],
    'tidak': ['ga', 'gak'],
    'oke' : ['ok'],
    'mudah': ['easy'],
    'besar': ['gede'],
    'kukuh': ['kokoh'],
    'senang' :['happy'],
    'saran' : ['rekomen', 'recommended'],
    'pas' : ['sesuai'],
    'sepadan' :['worth it'],
    'terlambat' : ['telat'],
    'kotor' : ['jorok'],
    'menunda' :['delay'],
    'melentur' : ['melenyot']
}

# Daftar kata-kata tidak relevan yang ingin dihapus
irrelevant_words = ['yang', 'barang', 'nya', 'botol', 'air', 'ini', 'itu']

# Persiapan stopwords
stop_words = set(stopwords.words('indonesian'))


def remove_emoji(text):
    return emoji.replace_emoji(text, replace='')  # Menghapus semua emoji

# Fungsi untuk membersihkan teks dan mengganti kata slang menjadi baku
def clean_text(text):
    text = text.lower()  # Lowercase semua teks
    text = remove_emoji(text)  # Hapus emoji di sini
    text = re.sub(r'\d+', '', text)  # Menghapus angka
    text = re.sub(r'[^\w\s]', '', text)  # Menghapus tanda baca

    # Gantikan kata slang dengan kata baku berdasarkan custom_dict
    for key, replacements in custom_dict.items():
        for word in replacements:
            # Gunakan regex untuk memastikan penggantian hanya dilakukan pada kata yang terpisah
            text = re.sub(r'\b' + re.escape(word) + r'\b', key, text)

    tokens = word_tokenize(text)  # Tokenisasi teks
    tokens = [word for word in tokens if word not in stop_words]  # Menghapus stopwords
    tokens = [word for word in tokens if word not in irrelevant_words]  # Menghapus kata irrelevan
    tokens = list(dict.fromkeys(tokens))  # Menghapus duplikasi dengan mengubah ke set dan kembali ke list
    return ' '.join(tokens)  # Gabungkan kembali token menjadi kalimat



# Terapkan fungsi untuk membersihkan teks di kolom 'Ulasan'
df['Cleaned'] = df['Ulasan'].apply(clean_text)

# Menampilkan hasil ulasan dan cleaned
df[['Ulasan', 'Cleaned']]

"""Bersihkan Duplikat"""

# Menampilkan data duplikat di data cleaned
duplikat = df['Cleaned'][df['Cleaned'].duplicated()]
duplikat

# Menghapus duplikat berdasarkan kolom 'Cleaned'
df = df.drop_duplicates(subset='Cleaned', keep='first').reset_index(drop=True)

# Menampilkan hasil sesudah duplikat dihapus
df['Cleaned']

"""# Labeling

Labeling Lexicon
- Positif > Negatif : Kelas Positif
- Positif < Negatif : Kelas Negatif
- Positif = Negatif : Kelas Netral
"""

# Load lexicon
df_positif = pd.read_csv("positif.csv", sep=";")
df_negatif = pd.read_csv("negatif.csv", sep=";")

df_positif.columns = df_positif.columns.str.strip().str.lower()
df_negatif.columns = df_negatif.columns.str.strip().str.lower()

df_positif['weight'] = df_positif['weight'].astype(int)
df_negatif['weight'] = df_negatif['weight'].astype(int)

# Gabungkan ke dictionary, tanpa menimpa nilai positif jika ada duplikat
lexicon_dict = {}

# Tambahkan kata dari lexicon positif
for _, row in df_positif.iterrows():
    lexicon_dict[row['word']] = row['weight']

# Tambahkan kata dari lexicon negatif jika belum ada di positif
for _, row in df_negatif.iterrows():
    if row['word'] not in lexicon_dict:
        lexicon_dict[row['word']] = -abs(row['weight'])

# Fungsi debug dengan detail skor
def assign_sentiment_score_debug(text):
    text = text.lower()
    tokens = text.split()  # tokenisasi sederhana
    score = 0
    details = []

    for token in tokens:
        weight = lexicon_dict.get(token)
        if weight is not None:
            score += weight
            details.append((token, weight))

    print(f"\nKalimat: {text}")
    print(f"Token ditemukan: {details}")
    print(f"Skor akhir: {score}")

    if score > 0:
        return "Positif"
    elif score < 0:
        return "Negatif"
    else:
        return "Netral"

# Terapkan fungsi scoring ke kolom cleaned
df['Label'] = df['Cleaned'].apply(assign_sentiment_score_debug)

# Baru tampilkan hasil
df[['Cleaned', 'Label']]

positif_data = df[df['Label'] == 'Positif'][['Cleaned', 'Label']]
positif_data

# Menampilkan hanya kolom 'Cleaned' dan 'Label' untuk kelas negatif
negatif_data = df[df['Label'] == 'Negatif'][['Cleaned', 'Label']]
negatif_data

# Menampilkan hanya kolom 'Cleaned' dan 'Label' untuk kelas Positif
netral_data = df[df['Label'] == 'Netral'][['Cleaned', 'Label']]
netral_data

# Jumlah Distribusi Sentimen
# Menghitung jumlah masing-masing label
label_counts = df['Label'].value_counts()
label_counts

# Membuat bar plot untuk distribusi sentimen
plt.figure(figsize=(8, 6))
sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis', hue=label_counts.index, dodge=False)  # Set hue and dodge

# Menambahkan judul dan label
plt.title('Distribusi Sentimen (Positif, Negatif, Netral)', fontsize=15)
plt.xlabel('Sentimen', fontsize=12)
plt.ylabel('Jumlah', fontsize=12)
plt.legend([],[],frameon=False) # Remove the legend

# Menampilkan plot
plt.show()

"""Visualisasi Word Cloud"""

all_text = ' '.join(df['Cleaned'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud - Semua Label', fontsize=20)
plt.axis('off')
plt.show()

def generate_wordcloud_from_label(df, label, ax):

    filtered_df = df[df['Label'] == label]
    text = ' '.join(filtered_df['Cleaned'])

    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    ax.imshow(wordcloud, interpolation='bilinear')
    ax.set_title(f'Word Cloud - {label}', fontsize=20)
    ax.axis('off')

# Buat figure dan axis untuk 3 label
fig, axes = plt.subplots(1, 3, figsize=(20, 10))

# Panggil fungsi untuk setiap label
generate_wordcloud_from_label(df, 'Positif', axes[0])
generate_wordcloud_from_label(df, 'Negatif', axes[1])
generate_wordcloud_from_label(df, 'Netral', axes[2])

plt.tight_layout()
plt.show()

"""# Ekstraksi Fitur

Pisah variabel X dan y
"""

# Menyusun X dan y
X = df['Cleaned']  # Fitur dari kolom 'Cleaned'
y = df['Label']  # Target dari kolom 'Label'

# Menggunakan TfidfVectorizer untuk ekstraksi fitur
vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(X)

# Visualisasi TF-IDF
X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())

# Gabungkan dengan label
df_tfidf = pd.concat([X_tfidf_df, y], axis=1)

# Tampilkan hasilnya
df_tfidf

"""SMOTE Oversampling untuk mengatasi data yang tidak seimbang"""

# Terapkan SMOTE untuk oversampling setelah TF-IDF
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_tfidf, y)

# Cek distribusi label setelah SMOTE
# Menghitung jumlah masing-masing label setelah SMOTE
label_counts_smote = pd.Series(y_smote).value_counts()
label_counts_smote

# Membuat bar plot untuk distribusi label setelah oversampling
plt.figure(figsize=(8, 6))
sns.barplot(x=label_counts_smote.index, y=label_counts_smote.values, palette='viridis', hue=label_counts_smote.index, dodge=False)

# Menambahkan judul dan label
plt.title('Distribusi Label Setelah Oversampling dengan SMOTE', fontsize=15)
plt.xlabel('Label Sentimen', fontsize=12)
plt.ylabel('Jumlah Sampel', fontsize=12)
plt.legend([],[],frameon=False) # Remove the legend

# Menampilkan plot
plt.show()

# Label Encoding untuk y_resampled
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y_smote)

# Buat DataFrame mapping label ke angka
label_mapping_df = pd.DataFrame({
    'Label': encoder.classes_,
    'Encoding': range(len(encoder.classes_))
})

# Tampilkan
label_mapping_df

# Membagi data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(X_smote, y_encoded, random_state=42, stratify=y_encoded)

"""# Modeling

# Evaluation
"""

# Membuat dan melatih model SVM
model = SVC(random_state=42)
model.fit(X_train, y_train)

# Prediksi Neural Network pada data uji
y_pred = model.predict(X_test)

# Evaluasi model Neural Network
print("\n=== Evaluasi Model ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

joblib.dump(model, 'model.pkl')
joblib.dump(encoder, 'encoder.pkl')
joblib.dump(vectorizer, 'vectorizer.pkl')